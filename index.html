<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>FSL Client-Side AI (Mobile Optimized)</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"></script>

    <style>
        body { background: #222; color: white; font-family: sans-serif; display: flex; flex-direction: column; align-items: center; justify-content: center; height: 100vh; margin: 0; }
        
        /* Responsive Container */
        #container { position: relative; width: 100%; max-width: 480px; aspect-ratio: 4/3; border: 4px solid #444; border-radius: 12px; overflow: hidden; background: #000; }
        
        #videoElement { display: none; }
        #canvasElement { width: 100%; height: 100%; transform: scaleX(-1); }

        #overlay { position: absolute; top: 10px; left: 10px; background: rgba(0, 0, 0, 0.6); padding: 10px; border-radius: 5px; text-align: left; pointer-events: none; }
        
        #status-display { position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%); font-size: 32px; font-weight: bold; text-shadow: 2px 2px 4px #000; z-index: 20; pointer-events: none; text-align: center;}
        
        #progress-container { position: absolute; bottom: 0; left: 0; width: 100%; height: 15px; background: rgba(50, 50, 50, 0.8); }
        #progress-bar { height: 100%; width: 0%; background: #00ff00; transition: width 0.1s linear; }

        #prediction-box { margin-top: 15px; font-size: 18px; padding: 15px; background: #333; border-radius: 8px; width: 90%; max-width: 480px; text-align: center; }
        .highlight { color: #0f0; font-weight: bold; }
    </style>
</head>
<body>

    <h3>FSL AI (Mobile Fast)</h3>

    <div id="container">
        <video id="videoElement" playsinline></video>
        <canvas id="canvasElement"></canvas>
        
        <div id="overlay">
            <div>Conf: <span id="conf">0.00</span></div>
            <div>Sign: <span id="curr" class="highlight">-</span></div>
        </div>

        <div id="status-display">LOADING...</div>

        <div id="progress-container">
            <div id="progress-bar"></div>
        </div>
    </div>

    <div id="prediction-box">Sentence: <span id="sentence">Waiting...</span></div>

    <script>
        const videoElement = document.getElementById('videoElement');
        const canvasElement = document.getElementById('canvasElement');
        const canvasCtx = canvasElement.getContext('2d');
        const progressBar = document.getElementById('progress-bar');
        const statusDisplay = document.getElementById('status-display');
        const currSpan = document.getElementById('curr');
        const confSpan = document.getElementById('conf');
        const sentSpan = document.getElementById('sentence');

        let model;
        let labels = [];
        let sequence = [];
        let sentence = [];
        
        // Configuration
        const SEQUENCE_LENGTH = 60; 
        const THRESHOLD = 0.8;

        // State Machine
        const STATE_PREP = 0;
        const STATE_RECORD = 1;
        const STATE_RESULT = 2;
        let currentState = STATE_PREP;
        let phaseStartTime = 0;

        // --- 1. Load Model & Labels ---
        async function loadAssets() {
            statusDisplay.innerText = "LOADING MODEL...";
            try {
                // Load TFJS Model
                model = await tf.loadLayersModel('model/model.json');
                console.log("Model Loaded!");

                // Load Labels
                const response = await fetch('js/labels.json');
                labels = await response.json();
                console.log("Labels Loaded!", labels);
                
                statusDisplay.innerText = "STARTING CAM...";
                startCamera();
            } catch (err) {
                console.error("Error loading assets:", err);
                statusDisplay.innerText = "ERROR LOADING";
            }
        }

        // --- 2. Landmark Extraction (Adapted for MediaPipe Hands) ---
        // This maps the dynamic "multiHandLandmarks" list back to your fixed [Left, Right] format
        function extractKeypoints(results) {
            // Initialize empty arrays (zeros)
            let lh = new Array(21 * 3).fill(0);
            let rh = new Array(21 * 3).fill(0);

            if (results.multiHandLandmarks && results.multiHandedness) {
                for (let i = 0; i < results.multiHandLandmarks.length; i++) {
                    const label = results.multiHandedness[i].label; // "Left" or "Right"
                    const landmarks = results.multiHandLandmarks[i];
                    
                    // Flatten x, y, z
                    const flatPoints = [];
                    for (const lm of landmarks) {
                        flatPoints.push(lm.x, lm.y, lm.z);
                    }

                    // Map to correct array (Note: MediaPipe labels are sometimes mirrored)
                    if (label === 'Left') {
                        lh = flatPoints;
                    } else {
                        rh = flatPoints;
                    }
                }
            }
            return [...lh, ...rh]; // Concatenated 126 features
        }

        // --- 3. MediaPipe Setup ---
        function onResults(results) {
            // Draw on Canvas
            canvasElement.width = videoElement.videoWidth;
            canvasElement.height = videoElement.videoHeight;
            canvasCtx.save();
            canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
            canvasCtx.drawImage(results.image, 0, 0, canvasElement.width, canvasElement.height);
            
            // Draw Hand Connectors
            if (results.multiHandLandmarks) {
                for (const landmarks of results.multiHandLandmarks) {
                    drawConnectors(canvasCtx, landmarks, HAND_CONNECTIONS, {color: '#00FF00', lineWidth: 3});
                    drawLandmarks(canvasCtx, landmarks, {color: '#FF0000', lineWidth: 1, radius: 3});
                }
            }
            canvasCtx.restore();

            // --- MAIN LOGIC LOOP ---
            const now = Date.now();
            if (phaseStartTime === 0) phaseStartTime = now;
            const elapsed = now - phaseStartTime;

            if (currentState === STATE_PREP) {
                const progress = Math.min(elapsed / 2000, 1.0) * 100;
                progressBar.style.width = progress + "%";
                progressBar.style.backgroundColor = "yellow";
                statusDisplay.innerText = "GET READY";
                statusDisplay.style.color = "yellow";

                if (elapsed >= 2000) {
                    currentState = STATE_RECORD;
                    phaseStartTime = Date.now();
                    sequence = [];
                }

            } else if (currentState === STATE_RECORD) {
                const progress = Math.min(elapsed / 2000, 1.0) * 100;
                progressBar.style.width = progress + "%";
                progressBar.style.backgroundColor = "red";
                statusDisplay.innerText = "RECORDING";
                statusDisplay.style.color = "red";

                // COLLECT DATA
                const keypoints = extractKeypoints(results);
                sequence.push(keypoints);

                if (elapsed >= 2000) {
                    currentState = STATE_RESULT;
                    phaseStartTime = Date.now();
                    runPrediction(); 
                }

            } else if (currentState === STATE_RESULT) {
                const progress = Math.min(elapsed / 2000, 1.0) * 100;
                progressBar.style.width = (100 - progress) + "%";
                progressBar.style.backgroundColor = "#00ff00";
                
                statusDisplay.style.color = "#00ff00";

                if (elapsed >= 2000) {
                    currentState = STATE_PREP;
                    phaseStartTime = Date.now();
                }
            }
        }

        // --- 4. Prediction Logic (Unchanged) ---
        async function runPrediction() {
            if (sequence.length < 5) return; 

            // Pad sequence to exactly 60 frames
            let finalSeq = [];
            if (sequence.length >= SEQUENCE_LENGTH) {
                finalSeq = sequence.slice(sequence.length - SEQUENCE_LENGTH);
            } else {
                const paddingCount = SEQUENCE_LENGTH - sequence.length;
                const zeroFrame = new Array(126).fill(0); // 21*3*2
                for(let i=0; i<paddingCount; i++) finalSeq.push(zeroFrame);
                finalSeq = finalSeq.concat(sequence);
            }

            const inputTensor = tf.tensor3d([finalSeq]);

            const prediction = model.predict(inputTensor);
            const values = await prediction.data();
            const maxVal = Math.max(...values);
            const maxIndex = values.indexOf(maxVal);

            const predictedSign = labels[maxIndex];
            confSpan.innerText = maxVal.toFixed(2);

            if (maxVal > THRESHOLD) {
                currSpan.innerText = predictedSign;
                statusDisplay.innerText = predictedSign;
                
                if (sentence.length === 0 || predictedSign !== sentence[sentence.length - 1]) {
                    sentence.push(predictedSign);
                    if (sentence.length > 5) sentence.shift();
                    sentSpan.innerText = sentence.join(" ");
                }
            } else {
                currSpan.innerText = "...";
                statusDisplay.innerText = "LOW CONFIDENCE";
            }
            
            inputTensor.dispose();
            prediction.dispose();
        }

        // --- 5. Optimized Start ---
        // Using "Hands" instead of "Holistic"
        const hands = new Hands({locateFile: (file) => {
            return `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`;
        }});
        
        hands.setOptions({
            maxNumHands: 2,
            modelComplexity: 0, // 0 = LITE (Fastest for Mobile), 1 = FULL
            minDetectionConfidence: 0.5,
            minTrackingConfidence: 0.5
        });
        
        hands.onResults(onResults);

        function startCamera() {
            const camera = new Camera(videoElement, {
                onFrame: async () => {
                    await hands.send({image: videoElement});
                },
                // Lower resolution for faster processing on phone
                width: 480, 
                height: 360 
            });
            camera.start();
        }

        loadAssets();

    </script>
</body>
</html>