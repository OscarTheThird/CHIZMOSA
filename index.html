<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>FSL Client-Side AI</title>
    <!-- 1. Import TensorFlow.js -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest/dist/tf.min.js"></script>
    <!-- 2. Import MediaPipe -->
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/holistic/holistic.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"></script>

    <style>
        body { background: #222; color: white; font-family: sans-serif; display: flex; flex-direction: column; align-items: center; justify-content: center; height: 100vh; margin: 0; }
        
        #container { position: relative; width: 640px; height: 480px; border: 4px solid #444; border-radius: 12px; overflow: hidden; background: #000; }
        
        /* Video is hidden, we draw on canvas */
        #videoElement { display: none; }
        #canvasElement { width: 100%; height: 100%; transform: scaleX(-1); }

        #overlay { position: absolute; top: 10px; left: 10px; background: rgba(0, 0, 0, 0.6); padding: 10px; border-radius: 5px; text-align: left; }
        
        #status-display { position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%); font-size: 48px; font-weight: bold; text-shadow: 2px 2px 4px #000; z-index: 20; }
        
        #progress-container { position: absolute; bottom: 0; left: 0; width: 100%; height: 20px; background: rgba(50, 50, 50, 0.8); }
        #progress-bar { height: 100%; width: 0%; background: #00ff00; transition: width 0.1s linear; }

        #prediction-box { margin-top: 20px; font-size: 24px; padding: 20px; background: #333; border-radius: 8px; width: 600px; text-align: center; }
        .highlight { color: #0f0; font-weight: bold; }
    </style>
</head>
<body>

    <h1>FSL Client-Side AI</h1>

    <div id="container">
        <video id="videoElement"></video>
        <canvas id="canvasElement"></canvas>
        
        <div id="overlay">
            <div>Confidence: <span id="conf">0.00</span></div>
            <div>Current Sign: <span id="curr" class="highlight">-</span></div>
        </div>

        <div id="status-display">LOADING...</div>

        <div id="progress-container">
            <div id="progress-bar"></div>
        </div>
    </div>

    <div id="prediction-box">Sentence: <span id="sentence">Waiting...</span></div>

    <script>
        const videoElement = document.getElementById('videoElement');
        const canvasElement = document.getElementById('canvasElement');
        const canvasCtx = canvasElement.getContext('2d');
        const progressBar = document.getElementById('progress-bar');
        const statusDisplay = document.getElementById('status-display');
        const currSpan = document.getElementById('curr');
        const confSpan = document.getElementById('conf');
        const sentSpan = document.getElementById('sentence');

        let model;
        let labels = [];
        let sequence = [];
        let sentence = [];
        
        // Configuration
        const SEQUENCE_LENGTH = 60; 
        const THRESHOLD = 0.8;

        // State Machine
        const STATE_PREP = 0;
        const STATE_RECORD = 1;
        const STATE_RESULT = 2;
        let currentState = STATE_PREP;
        let phaseStartTime = 0;

        // --- 1. Load Model & Labels ---
        async function loadAssets() {
            statusDisplay.innerText = "LOADING MODEL...";
            try {
                // Load TFJS Model
                model = await tf.loadLayersModel('model/model.json');
                console.log("Model Loaded!");

                // Load Labels
                const response = await fetch('js/labels.json');
                labels = await response.json();
                console.log("Labels Loaded!", labels);
                
                statusDisplay.innerText = "GET READY";
                startCamera();
            } catch (err) {
                console.error("Error loading assets:", err);
                statusDisplay.innerText = "ERROR LOADING";
            }
        }

        // --- 2. Landmark Extraction Logic (Matches Python) ---
        function extractKeypoints(results) {
            // Function to Flatten and zero-pad if missing
            const getHand = (landmarks) => {
                if (landmarks) {
                    let flat = [];
                    for (const lm of landmarks) {
                        flat.push(lm.x, lm.y, lm.z);
                    }
                    return flat;
                } else {
                    return new Array(21 * 3).fill(0);
                }
            };

            const lh = getHand(results.leftHandLandmarks);
            const rh = getHand(results.rightHandLandmarks);
            return [...lh, ...rh]; // Concatenate arrays
        }

        // --- 3. MediaPipe Setup ---
        function onResults(results) {
            // Draw on Canvas
            canvasElement.width = videoElement.videoWidth;
            canvasElement.height = videoElement.videoHeight;
            canvasCtx.save();
            canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
            canvasCtx.drawImage(results.image, 0, 0, canvasElement.width, canvasElement.height);
            
            // Draw Connectors
            drawConnectors(canvasCtx, results.leftHandLandmarks, HAND_CONNECTIONS, {color: '#CC0000', lineWidth: 5});
            drawLandmarks(canvasCtx, results.leftHandLandmarks, {color: '#00FF00', lineWidth: 2});
            drawConnectors(canvasCtx, results.rightHandLandmarks, HAND_CONNECTIONS, {color: '#00CC00', lineWidth: 5});
            drawLandmarks(canvasCtx, results.rightHandLandmarks, {color: '#FF0000', lineWidth: 2});
            canvasCtx.restore();

            // --- MAIN LOGIC LOOP ---
            const now = Date.now();
            if (phaseStartTime === 0) phaseStartTime = now;
            const elapsed = now - phaseStartTime;

            if (currentState === STATE_PREP) {
                const progress = Math.min(elapsed / 2000, 1.0) * 100;
                progressBar.style.width = progress + "%";
                progressBar.style.backgroundColor = "yellow";
                statusDisplay.innerText = "GET READY";
                statusDisplay.style.color = "yellow";

                if (elapsed >= 2000) {
                    currentState = STATE_RECORD;
                    phaseStartTime = Date.now();
                    sequence = []; // Clear buffer
                }

            } else if (currentState === STATE_RECORD) {
                const progress = Math.min(elapsed / 2000, 1.0) * 100;
                progressBar.style.width = progress + "%";
                progressBar.style.backgroundColor = "red";
                statusDisplay.innerText = "RECORDING";
                statusDisplay.style.color = "red";

                // COLLECT DATA
                const keypoints = extractKeypoints(results);
                sequence.push(keypoints);

                if (elapsed >= 2000) {
                    currentState = STATE_RESULT;
                    phaseStartTime = Date.now();
                    runPrediction(); // Trigger prediction at end of recording
                }

            } else if (currentState === STATE_RESULT) {
                const progress = Math.min(elapsed / 2000, 1.0) * 100;
                progressBar.style.width = (100 - progress) + "%";
                progressBar.style.backgroundColor = "#00ff00";
                
                // Display is handled by runPrediction, text stays visible
                statusDisplay.style.color = "#00ff00";

                if (elapsed >= 2000) {
                    currentState = STATE_PREP;
                    phaseStartTime = Date.now();
                }
            }
        }

        // --- 4. Prediction Logic ---
        async function runPrediction() {
            if (sequence.length < 5) return; // Not enough data

            // Pad sequence to exactly 60 frames
            // Note: TFJS needs a tensor of shape [1, 60, 126]
            
            // 1. Fill or Trim to 60
            let finalSeq = [];
            if (sequence.length >= SEQUENCE_LENGTH) {
                finalSeq = sequence.slice(sequence.length - SEQUENCE_LENGTH);
            } else {
                // Pad with zeros at the start
                const paddingCount = SEQUENCE_LENGTH - sequence.length;
                const zeroFrame = new Array(126).fill(0);
                for(let i=0; i<paddingCount; i++) finalSeq.push(zeroFrame);
                finalSeq = finalSeq.concat(sequence);
            }

            // 2. Convert to Tensor
            const inputTensor = tf.tensor3d([finalSeq]);

            // 3. Predict
            const prediction = model.predict(inputTensor);
            const values = await prediction.data();
            const maxVal = Math.max(...values);
            const maxIndex = values.indexOf(maxVal);

            // 4. Update UI
            const predictedSign = labels[maxIndex];
            confSpan.innerText = maxVal.toFixed(2);

            if (maxVal > THRESHOLD) {
                currSpan.innerText = predictedSign;
                statusDisplay.innerText = predictedSign;
                
                // Sentence Logic
                if (sentence.length === 0 || predictedSign !== sentence[sentence.length - 1]) {
                    sentence.push(predictedSign);
                    if (sentence.length > 5) sentence.shift();
                    sentSpan.innerText = sentence.join(" ");
                }
            } else {
                currSpan.innerText = "...";
                statusDisplay.innerText = "LOW CONFIDENCE";
            }
            
            // Clean up tensor memory
            inputTensor.dispose();
            prediction.dispose();
        }

        // --- 5. Start ---
        const holistic = new Holistic({locateFile: (file) => {
            return `https://cdn.jsdelivr.net/npm/@mediapipe/holistic/${file}`;
        }});
        
        holistic.setOptions({
            modelComplexity: 1,
            smoothLandmarks: true,
            minDetectionConfidence: 0.5,
            minTrackingConfidence: 0.5
        });
        
        holistic.onResults(onResults);

        function startCamera() {
            const camera = new Camera(videoElement, {
                onFrame: async () => {
                    await holistic.send({image: videoElement});
                },
                width: 320,
                height: 240
            });
            camera.start();
        }

        // Init
        loadAssets();

    </script>
</body>
</html>